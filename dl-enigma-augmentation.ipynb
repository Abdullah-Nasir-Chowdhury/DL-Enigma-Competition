{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67836,"databundleVersionId":7527339,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# basic libraries ...\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ultrlytics ...\n\n!pip install ultralytics -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# yolo ...\n\nfrom ultralytics import YOLO","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete all the files and directories recursively in the current working directory ...\n\n!rm -rf *\n\n# make directory ...\n\n!mkdir /kaggle/working/datasets\n!mkdir /kaggle/working/datasets/badodd\n!mkdir /kaggle/working/datasets/badodd/labels\n!mkdir /kaggle/working/datasets/badodd/labels/train\n!mkdir /kaggle/working/datasets/badodd/images\n!mkdir /kaggle/working/datasets/badodd/images/train\n!mkdir /kaggle/working/datasets/badodd/images/test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  reference original files without duplicating their content ...\n\ndef all_files_in_folder_symlink(source_dir, target_dir):\n    files = os.listdir(source_dir)\n    \n    for file in tqdm(files):\n        source_file = os.path.join(source_dir, file)\n        target_file = os.path.join(target_dir, file)\n        os.symlink(source_file, target_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# symbolic link function as above ...\n\nall_files_in_folder_symlink(\"/kaggle/input/dl-enigma-10-sust-cse-carnival-2024/dlenigma1/BadODD/labels/train\",\"/kaggle/working/datasets/badodd/labels/train\")\nall_files_in_folder_symlink(\"/kaggle/input/dl-enigma-10-sust-cse-carnival-2024/dlenigma1/BadODD/images/train\",\"/kaggle/working/datasets/badodd/images/train\")\nall_files_in_folder_symlink(\"/kaggle/input/dl-enigma-10-sust-cse-carnival-2024/dlenigma1/BadODD/images/test\",\"/kaggle/working/datasets/badodd/images/test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# working directory structure ...\n\n!tree -d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model part ...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration file that tells YOLO where to find the dataset and what objects to detect ...\n\nfile_content = \"\"\"\npath: /kaggle/working/datasets/badodd  # dataset root dir\ntrain: images/train  # train images (relative to 'path')\nval: images/train  # val images (relative to 'path')\ntest:  images/test\n\n# Classes\nnames:\n    0: auto_rickshaw\n    1: bicycle\n    2: bus\n    3: car\n    4: cart_vehicle\n    5: construction_vehicle\n    6: motorbike\n    7: person\n    8: priority_vehicle\n    9: three_wheeler\n    10: train\n    11: truck\n    12: wheelchair\n\"\"\"\n\nwith open(\"yolov8.yaml\", mode=\"w\") as f:\n    f.write(file_content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Weights & Biases ...\n\nimport wandb\nwandb.init(mode=\"disabled\")\n# wandb.init()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentations","metadata":{}},{"cell_type":"code","source":"# Helper functions:\ndef load_annotation(annotation_file):\n    annotations = []\n    with open(annotation_file, 'r') as file:\n        lines = file.readlines()\n        for line in lines:\n            # Parse the line to extract bounding box coordinates and class label\n            data = line.strip().split()\n            class_id = int(data[0])\n            x_center = float(data[1])\n            y_center = float(data[2])\n            width = float(data[3])\n            height = float(data[4])\n            annotations.append([class_id, x_center, y_center, width, height])\n    return annotations\n\ndef save_annotation(annotation_file, annotations):\n    with open(annotation_file, 'w') as file:\n        for box in annotations:\n            # Convert bounding box coordinates to string format and write to file\n            line = ' '.join([str(coord) for coord in box]) + '\\n'\n            file.write(line)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:07:18.142126Z","iopub.execute_input":"2024-02-05T09:07:18.142561Z","iopub.status.idle":"2024-02-05T09:07:18.152186Z","shell.execute_reply.started":"2024-02-05T09:07:18.142528Z","shell.execute_reply":"2024-02-05T09:07:18.151156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Load original image and annotation\noriginal_image_path = '/kaggle/input/dl-enigma-10-sust-cse-carnival-2024/dlenigma1/BadODD/images/train/sylhet4_9794.jpg'\noriginal_annotation_path = original_image_path.replace('images', 'labels').replace('.jpg', '.txt')\noriginal_image = cv2.imread(original_image_path)\noriginal_annotation = load_annotation(original_annotation_path)\nprint(original_annotation)\n\n# Apply image augmentation (e.g., rotation)\nangle = 15\nrows, cols = original_image.shape[:2]\nrotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\naugmented_image = cv2.warpAffine(original_image, rotation_matrix, (cols, rows))\n\n# Update bounding box coordinates in the annotation\n# Example: update bounding box coordinates for rotation\n# (Assuming the bounding box format is [x_center, y_center, width, height])\nupdated_annotation = []\nfor box in original_annotation:\n    class_id, x_center, y_center, width, height = box\n    \n    # Rotate bounding box coordinates\n    x_min = x_center - width / 2\n    y_min = y_center - height / 2\n    x_max = x_center + width / 2\n    y_max = y_center + height / 2\n\n    # Rotate bounding box coordinates\n    new_x_min = x_min * np.cos(np.radians(angle)) - y_min * np.sin(np.radians(angle))\n    new_y_min = x_min * np.sin(np.radians(angle)) + y_min * np.cos(np.radians(angle))\n    new_x_max = x_max * np.cos(np.radians(angle)) - y_max * np.sin(np.radians(angle))\n    new_y_max = x_max * np.sin(np.radians(angle)) + y_max * np.cos(np.radians(angle))\n\n    # Recalculate width and height\n    new_width = new_x_max - new_x_min\n    new_height = new_y_max - new_y_min\n\n    # Recalculate center coordinates\n    new_x_center = new_x_min + new_width / 2\n    new_y_center = new_y_min + new_height / 2\n\n    updated_annotation.append([class_id, new_x_center, new_y_center, new_width, new_height])\n\n    \n    \n    \n    # Rotate bounding box center coordinates\n#     new_x_center = x_center * np.cos(np.radians(angle)) - x_center * np.sin(np.radians(angle))\n#     new_y_center = y_center * np.sin(np.radians(angle)) + y_center * np.cos(np.radians(angle))\n#     updated_annotation.append([class_id, new_x_center, new_y_center, width, height])\n\n# Save augmented image and updated annotation\ncv2.imwrite('augmented_image.jpg', augmented_image)\nsave_annotation('updated_annotation.txt', updated_annotation)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:11:22.823004Z","iopub.execute_input":"2024-02-05T09:11:22.824282Z","iopub.status.idle":"2024-02-05T09:11:22.953052Z","shell.execute_reply.started":"2024-02-05T09:11:22.824226Z","shell.execute_reply":"2024-02-05T09:11:22.952032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Load augmented image\naugmented_image = cv2.imread('/kaggle/working/augmented_image.jpg')\n\n# Read the annotations from the text file\nannotations_file_path = '/kaggle/working/updated_annotation.txt'\nwith open(annotations_file_path, 'r') as f:\n    annotations_data = f.readlines()\n\n# Parse annotations and draw bounding boxes\nfor line in annotations_data:\n    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n    \n    # Convert YOLO format to bounding box coordinates\n    img_height, img_width = original_image.shape[:2]\n    x = (x_center - width / 2) * img_width\n    y = (y_center - height / 2) * img_height\n    w = width * img_width\n    h = height * img_height\n    \n    # Draw bounding box on the image\n    original_image = cv2.rectangle(augmented_image, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)\n\n# Convert BGR to RGB for Matplotlib\noriginal_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n\n# Display the image with bounding boxes using Matplotlib\nplt.figure()\nplt.imshow(original_image_rgb)\nplt.axis('off')\nplt.title('Image with Bounding Boxes')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:11:25.304675Z","iopub.execute_input":"2024-02-05T09:11:25.305090Z","iopub.status.idle":"2024-02-05T09:11:25.860445Z","shell.execute_reply.started":"2024-02-05T09:11:25.305060Z","shell.execute_reply":"2024-02-05T09:11:25.858586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\n# Read the original image\noriginal_image_path = '/kaggle/input/dl-enigma-10-sust-cse-carnival-2024/dlenigma1/BadODD/images/train/sylhet4_9794.jpg'\noriginal_image = cv2.imread(original_image_path)\n\n# Read the annotations from the text file\nannotations_file_path = original_image_path.replace('images','labels').replace('.jpg','.txt')\nwith open(annotations_file_path, 'r') as f:\n    annotations_data = f.readlines()\n\n# Parse annotations and draw bounding boxes\nfor line in annotations_data:\n    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n    \n    # Convert YOLO format to bounding box coordinates\n    img_height, img_width = original_image.shape[:2]\n    x = (x_center - width / 2) * img_width\n    y = (y_center - height / 2) * img_height\n    w = width * img_width\n    h = height * img_height\n    \n    # Draw bounding box on the image\n    original_image = cv2.rectangle(original_image, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)\n\n# Convert BGR to RGB for Matplotlib\noriginal_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n\n# Display the image with bounding boxes using Matplotlib\nplt.imshow(original_image_rgb)\nplt.axis('off')\nplt.title('Image with Bounding Boxes')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:33:49.024081Z","iopub.execute_input":"2024-02-05T06:33:49.024466Z","iopub.status.idle":"2024-02-05T06:33:49.431380Z","shell.execute_reply.started":"2024-02-05T06:33:49.024436Z","shell.execute_reply":"2024-02-05T06:33:49.430382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# yolov8 small model, configuration file and run the model in 3 epochs ... \n\n# Adam, AdamW, NAdam, RAdam, RMSProp, SGD, auto\nmodel_RMS = YOLO(\"yolov8n.pt\")\nmodel_RMS.train(\n    data=\"/kaggle/working/yolov8.yaml\", \n    \n    # Augmentation\n    perspective=0.3,\n    mosaic=0.8,\n    scale=0.3,\n    translate=0.4,\n    \n    # Training Parameters\n    epochs=100,\n    batch=16,\n#     workers=2,\n    val=True,\n    \n    # Optimization parameters\n    lr0=0.0001,\n#     patience=3,\n    optimizer=\"RMSProp\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get prediction boxes string according to the submission format ...\n\ndef get_prediction_string(boxes, scores, classes):\n    pred_strs = []\n    for i, score in enumerate(scores):\n        single_pred_str = \"\"\n        single_pred_str += str(float(classes[i])) + \" \" + str(float(score)) + \" \"\n        \n        x_center , y_center, width,height = boxes[i]\n        x1 = float(x_center) - (float(width) / 2)\n        y1 = float(y_center) - (float(height) / 2)\n        width = float(width)\n        height = float(height)\n        #single_pred_str += \" \".join(str(float(x)) for x in boxes[i])\n        single_pred_str +=  f\"{x1} {y1} {width} {height}\"\n        \n        pred_strs.append(single_pred_str)\n    ans = ','.join(map(str, pred_strs))\n    if len(ans):\n        return ans\n#     the solution metrics faield in case of a NaN, '' (empty). So, return \"0 0 0 0 0 0\" for NaN, '' (empty) string\n    return \"0 0 0 0 0 0\"\n\n# get the predcition in id, ImageID, PredictionString_pred foramt ...\n\ndef get_prediction_entry(i, filename, boxes, scores, classes):\n    return {\n        \"id\": i, # strating from 0 ...\n        \"ImageID\": filename.split('.')[0], # before the extension ...\n        \"PredictionString_pred\": get_prediction_string(boxes, scores, classes)\n    }\n\n# Directory path ...\ntest_directory = \"/kaggle/input/dl-enigma-10-sust-cse-carnival-2024/dlenigma1/BadODD/images/test\"\n\n# Load the model ...\nmodel = YOLO('/kaggle/working/runs/detect/train3/weights/best.pt')\n\n# do the inference ...\n\ndef predict_all_files(test_directory):\n    predictions = []\n    for i,filename in tqdm(enumerate(os.listdir(test_directory))):\n        if filename.endswith(\".jpg\"):\n            filepath = os.path.join(test_directory, filename)\n            results = model.predict(source=filepath, conf=0.50, verbose=False)\n            boxes = results[0].boxes.xywhn\n            scores = results[0].boxes.conf\n            classes = results[0].boxes.cls\n            prediction = get_prediction_entry(i, filename, boxes, scores, classes)\n            predictions.append(prediction)\n#             to csv format ...\n    predictions_df = pd.DataFrame(predictions)\n    predictions_df.to_csv(\"submission.csv\", index=False)\n\n# call the inference function ...\npredict_all_files(test_directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the submission dataframe ....\n\nsubmission_df = pd.read_csv('/kaggle/working/submission.csv')\nsubmission_df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.shape()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}